\appendix
\section{Basis Splines} \label{sec:basis_splines}

A common non-parametric method used in many statistical applications is basis splines. A spline function of order $k$, 
is a piece-wise polynomial of order $k$ polynomials stitched together from defined ``knot'' locations across the domain. 
They provide a useful and cheap way to interpolate generically smooth functions from a finite sampling of ``knot'' heights. 
Basis splines of order $k$ are a set of order $k$ polynomials that form a complete basis for any spline function of order $k$. 
Therefore, given an array of knot locations, $\mathbf{t}$ or knot vector, there exists a single unique linear combination of basis splines 
for every possible spline function interpolated from $\mathbf{t}$. To construct a basis of $n$ components and knots, $t_0$, $t_1$,...,$t_{i+k}$, 
we use the Cox-de Boor recursion formula \citep{deBoor78,monotone_regression_splines}. The recursion starts with the $k=0$ (constant) case and recursively constructs 
the basis components of higher orders. The base case and recursion relation that generates this particular basis are defined as:

\begin{equation}
    B_{i,0}(x | \mathbf{t}) = 
    \begin{cases}
        1, & \text{if } t_i \leq x < t_{i+1} \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

\begin{equation}
    B_{i,k+1}(x | \mathbf{t}) = \omega_{i,k}(x | \mathbf{t})B_{i,k}(x | \mathbf{t}) + \big[1-\omega_{i+1,k}(x | \mathbf{t})\big] B_{i+1,k}(x | \mathbf{t})
\end{equation}

\begin{equation}
    \omega_{i,k}(x | \mathbf{t}) =
    \begin{cases}
        \frac{x-t_i}{t_{i+k}-t_i}, & t_{i+k} \neq t_i \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

\noindent This is known as the ``B-Spline'' basis after it's inventor de Boor \citep{deBoor78}. The power of basis splines
comes from the fact that one only has to do the somewhat-expensive interpolation once for each set of points at which the spline is evaluated. 
This provides a considerable computational speedup as each evaluation of the spline function becomes a simpler operation: a dot product of a 
matrix and a vector. This straightforward operation is also ideal for optimizations from the use of GPU accelerators, 
enabling our Markov chain Monte Carlo (MCMC) based analyses, often with hundreds of parameters, to converge in an hour or less. 
Basis splines can easily be generalized to their two-dimensional analog, producing tensor product basis splines that, 
with this computational advantage, allow for high fidelity modeling of two-dimensional spline functions.

\begin{figure}[ht!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/spline_basis_plot.pdf}
        \caption{Plot showing a ``proper'' (see appendix \ref{sec:psplines}) normalized B-Spline basis of order 3 (cubic) with 20 degrees of freedom and equal weights for each component. 
        In black, we show the resulting spline function given equal weights and denote the location of the knots with gray x's.}
        \label{fig:spline_basis}
    \end{centering}
    \script{spline_basis_plot.py}
\end{figure}

Another important feature of basis splines is that under appropriate prior conditions, one can alleviate sensitivities to arbitrarily 
chosen prior specifications that splines commonly struggle with. Previous studies using splines had to perform multiple analyses, varying the 
number of spline knots, then either marginalized over the models or used model comparisons to motivate the best choice \citep{Edelman_2022ApJ}. 
We can avoid this step with the use of penalized splines (or P-Splines) \citep{eilers2021practical,BayesianPSplines,Jullion2007RobustSO}, 
where one adds a smoothing prior comprised of Gaussian distributions on the differences between neighboring basis spline coefficients. 
This allows for knots to be densely populated across the domain without the worry of extra variance in the inferred spline functions. 
When also fitting the scale of the smoothing prior (i.e. the width of the Gaussian distributions on the differences), the data will inform the model 
of the preferred the scale of smoothing required. We discuss the details of our smoothing prior implementation in more detail in the next section, 
Appendix \ref{sec:psplines}, following with our specific prior and basis choices for each model in Appendix \ref{sec:modelpriors}.

\section{Penalized Splines and Smoothing Priors}\label{sec:psplines}

Spline functions have been shown to be sensitive to the chosen number of knots, and their locations or spacing \citep{deBoor78}. 
Adding more knots increases the a priori variance in the spline function, while the space between knots can limit the 
resolution of features in the data the spline is capable of resolving. To ensure your spline based model is flexible enough 
one would want to add as many knots as densely as possible, but this comes with unwanted side effect of larger variance imposed by your model. 
This can be fixed with the use of penalized splines (P-Spline) in which one applies a prior or regularization term 
to the likelihood based on the difference of adjacent knot coefficients \citep{eilers2021practical}. The linear combination of spline basis components 
or the resulting spline function is flat when the basis coefficients are equal (see Figure \ref{fig:spline_basis}). By penalizing the likelihood as 
the differences between adjacent knot coefficients get larger, one gets a smoothing effect on the spline function \citep{eilers2021practical}. 
With hierarchical Bayesian inference as our statistical framework, we formulate the penalized likelihood of \citet{eilers2021practical}'s P-Splines with 
their Bayesian analog \citep{BayesianPSplines}. The Bayesian P-Spline prior places Gaussian distributions over the $r$-th order differences of the 
coefficients \citep{BayesianPSplines,Jullion2007RobustSO}. This is also sometimes referred to as a Gaussian random walk prior, and is similar in spirit to a Gaussian process prior used to regularize or smooth histogram bin heights as done in other non-parametric population studies \citep{Mandel_2016,o3b_astro_dist}. 
For a spline basis with $n$ degree's of freedom, and a difference penalty of order of $r$ (see \citet{eilers2021practical}), 
the smoothing prior on our basis spline coefficients, $\bm{c}$ is defined as:

\begin{eqnarray}
\bm{c} \sim \mathcal{N}(0, \sigma) \\
p(\bm{c} | \tau_\lambda) \propto \exp \big[ -\frac{1}{2} \tau_\lambda \bm{c}^{\mathrm{T}} \bm{D}_{r}^{\mathrm{T}} \bm{D}_r \bm{c}  \big] 
\end{eqnarray}

\noindent Above $\bm{D}_r$ is the order-$r$ difference matrix, of shape $(n-r \times n)$, and $\mathcal{N}(0,\sigma)$ a Gaussian distribution with zero mean 
and standard deviation, $\sigma$. This smoothing prior removes the strong dependence on number and location of knots that arises with using splines. 
The $\tau_\lambda$ controls the ``strength'' of the smoothing, or the inverse variance of the Gaussian priors on knot differences. We 
place uniform priors on $\tau_\lambda$ marginalize over this smoothing scale hyperparameter to let the data inform the optimal scale needed.
When there are a very large number of knots, such that your domain is densely populated with basis coefficients, this allows the freedom for the model to find the smoothing 
scale that the data prefers. 

%One downside of this prior specification as noted in \citet{BayesianPSplines} and \citet{Jullion2007RobustSO}, is the dependence on 
%one's choice of value for the $b$ hyperparameter in the Gamma distribution for $\tau_\lambda$. The models introduced in \citet{Jullion2007RobustSO} show how 
%to alleviate this. W model $\tau_\lambda$ with a mixture model composed of multiple Gamma distributions each with varying values for $b$. 
%With this we now have the full prior on $\tau_\lambda$ as:

%\begin{align}
%    \bm{p} \sim \mathcal{D}(m) \\
%    p(\tau_\lambda | \bm{p}) = \sum_i^m p_i \Gamma(1, b_i)
%\end{align}

%\noindent Above $\mathcal{D}(m)$, denotes a Dirichlet distribution with $m$ degrees of freedom and ones as the concentration parameters, 
%and the $b_i$'s are $m$ different b shape parameters for the Gamma distribution.

%Additionally one can implement a spatially adaptive smoothing prior which allows for a smoothing prior
%that can smoothly evolve over the domain of your spline function. This is done in practice by having the $\tau_\lambda$ parameter evolve smoothly
%across the knots. \bruce{CITE BAYESIAN PSPLINES}, formulates a spatially adaptive prior in this way by adding in $n-r-1$ additional parameters that
%effectively modulate the $\tau_\lambda$ across each difference. We now replace our prior on the coeficients with:

%\begin{eqnarray}
%\bm{\lambda} \sim \mathcal{G}(\omega, \omega), \\
%\bm{\Lambda} \equiv \mathrm{diag}\big(1, \lambda_0,...,\lambda_{n-r-1}\big), \\
%p(\bm{c} | \tau_\lambda) \propto \exp \big[ -\frac{1}{2} \tau_\lambda \bm{c}^{\mathrm{T}} \bm{D_r}^{\mathrm{T}} \bm{\Lambda} \bm{D_r} \bm{c} \big] e^{-n\bm{c}}
%\end{eqnarray}

%Where above the matrix $\bm{\Lambda}$ a diagonal matrix of shape $(n-r \times n-r)$, and $\omega$ an arbitrarily small value such that the Gamma distribution 
%prior on $\bm{\lambda}$ has a mean of 1 and an arbitrarily large variance. This enforces a smooth evolution of the smoothing penalty 
%across the domain that is being analyzed. This type of prior is especiially important when modeling distributions that span large orders of magnitude like 
%the power-law like mass distributions that are commonly used. 

This prior is imparting a natural attraction of the coefficients closer to each other in order to smooth the spline function, so one 
must ensure that the spline function is in fact flat given all equal coefficients. There needs to be $n+k+1$ knots to construct an order-k 
basis with n degrees of freedom. Some studies place knots on top of each other at hard parameter boundaries \citep{deBoor78,monotone_regression_splines}, 
which may seem motivated, but this violates the above condition necessary for the P-Spline prior. We follow the distinction in \citet{eilers2021practical} 
that such a smoothing prior is only valid with ``proper'' spline bases. A proper basis is where all $n+k+1$ knots are evenly and equally spaced, 
see Figure \ref{fig:spline_basis}, as opposed to stacking them at the bounds.

\section{Hierarchical Bayesian Inference} \label{sec:hierarchical_inference}

% This appendix goes over in detail the hierarchical bayeisan inference framework
We use hierarchical Bayesian inference to infer the population properties of CBCs. We want to infer the number density of merging compact binaries  
in the universe and how this can change with their masses, spins, etc. Often times it is useful to formulate the question in terms of the 
merger rates which is the number of mergers per $Gpc^{3}$ co-moving volume per year. For a set of hyperparameters, $\Lambda$, $\lambda$, and overall 
merger rate, $\mathcal{R}$, we write the overall number density of BBH mergers in the universe as: 

\begin{equation} \label{number_density}
     \frac{dN(\theta, z | \mathcal{R}, \Lambda, \lambda)}{d\theta dz} = \frac{dV_c}{dz}\bigg(\frac{T_\mathrm{obs}}{1+z}\bigg) \frac{d\mathcal{R}(\theta, z | \mathcal{R}_0, \Lambda, \lambda)}{d\theta} = \mathcal{R} p(\theta | \Lambda) p(z | \lambda)
\end{equation}

\noindent
where up above, we denote the co-moving volume element as $dV_c$ \citep{hogg_cosmo}, and $T_\mathrm{obs}$ as the observing time period that produced the 
catalog with the related factor of $1+z$ converting this detector-frame time to source-frame. We assume a Lambda CDM cosmology using 
the cosmological parameters from \citet{Planck2015}. We model the merger rate evolving with redshift following a power law distribution: 
$p(z|\lambda) \propto \frac{dV_c}{dz}\frac{1}{1+z}(1+z)^\lambda$ \citep{Fishbach_2018redshift}. When integrating equation \ref{number_density} across all $\theta$
and out to some maximum redshift, $z_\mathrm{max}$, we get the total number of CBCs in the universe out to that redshift. We follow previous notations, \
letting $\{d_i\}$ represent the set of data from $N_\mathrm{obs}$ CBCs observed with gravitational waves. The merger rate is then described as an inhomogeneous 
Poisson process and after imposing the usual log-uniform prior on the merger rate, we marginalize over the merger rate, $\mathcal{R}$, and arrive at the posterior
distribution of our hyperparameters, $\Lambda$ \citep{Mandel_2019, Vitale_2021}.

\begin{equation}
    p\left(\Lambda, \lambda | \{d_i\}\right) \frac{p(\Lambda)p(\lambda)}{\xi(\Lambda,\lambda)^{N_\mathrm{obs}}} \prod_{i=1}^{N_\mathrm{obs}} \bigg[ \frac{1}{K_i} \sum_{j=1}^{K_i} \frac{p(\theta^{i,j}|\Lambda)p(z^{i,j}|\lambda)}{\pi(\theta, z^{i,j})} \bigg]
\end{equation}

\noindent
where above, we replaced the integrals over each event's likelihood with ensemble averages over $K_i$ posterior samples. Above, $j$
indexes the $K_i$ posterior samples from each event and $\pi(\theta, z)$ is the default prior used by parameter estimations that 
produced the posterior samples for each event. In the analyses of GWTC-3, either the default prior used was uniform in detector frame masses, 
component spins and Euclidean volume or the posterior samples were re-weighted to such a prior before using them in our analysis. 
The corresponding prior evaluated in the parameters we hierarchically model, i.e. source frame primary mass, mass ratio, component spins and redshift is:

\begin{equation}
    \pi(m_1, q, a_1, a_2, \cos{\theta_1}, \cos{\theta_2}, z) \propto \frac{1}{4} m_1 (1+z)^2 D_L^2(z) \frac{dD_L}{dz}
\end{equation}

\noindent Above, $D_L$ is the luminosity distance. To carefully incorporate selection effects to our model we need to quantify the detection efficiency,
$\xi(\Lambda, \lambda)$, of the search pipelines that were used to create GWTC-3, at a given population distribution described by $\Lambda$ and $\lambda$.
 
\begin{equation}
     \xi(\Lambda, \lambda) = \int d\theta dz P_\mathrm{det}(\theta, z)p(\theta | \Lambda) p(z | \lambda)
\end{equation}
 
\noindent
To estimate this integral we use a software injection campaign where gravitational waveforms from a large population of simulated sources. 
These simulated waveforms are put into real detector data, and then this data is evaluated with the same search pipelines that were used to 
produce the catalog we are analyzing. With these search results in hand, we use importance sampling and evaluate the integral 
with the Monte Carlo sum estimate $\mu$, and its corresponding variance and effective number of samples:

\begin{equation} \label{xi}
     \xi(\Lambda, \lambda) \approx \mu(\Lambda, \lambda) \frac{1}{N_\mathrm{inj}} \sum_{i=1}^{N_\mathrm{found}} \frac{p(\theta^i | \Lambda) p(z^i | \lambda)}{p_\mathrm{inj}(\theta, z^i)}
\end{equation}

\begin{equation}
    \sigma^2(\Lambda, \lambda) \equiv \frac{\mu^2(\Lambda, \lambda)}{N_\mathrm{eff}} \simeq \frac{1}{N^2_\mathrm{inj}} \sum_{i=1}^{N_\mathrm{found}} \bigg[\frac{p(\theta | \Lambda) p(z | \lambda)}{p_\mathrm{inj}(\theta, z)}\bigg]^2 - \frac{\mu^2(\Lambda, \lambda)}{N_\mathrm{inj}}
\end{equation}

\noindent
where the sum is only over the $N_\mathrm{found}$ injections that were successfully detected out of $N_\mathrm{inj}$ total injections, 
and $p_\mathrm{inj}(\theta, z)$ is the reference distribution from which the injections were drawn. We use the LVK released injection sets that describe the 
detector sensitivities over the first, second and third observing runs \citep{o1o2o3_injection_sets}. Additionally, we follow the procedure 
outlined in \citet{Farr_2019} to marginalize the uncertainty in our estimate of $\xi(\Lambda, \lambda)$, in which we verify that $N_\mathrm{eff}$ is 
sufficiently high after re-weighting the injections to a given population (i.e. $N_\mathrm{eff} > 4N_\mathrm{obs}$). 
The total hyper-posterior marginalized over the merger rate and the uncertainty in the Monte Carlo integral calculating $\xi(\Lambda, \lambda)$ \citep{Farr_2019}, as:

\begin{equation}\label{importance-posterior}
    \log p\left(\Lambda, \lambda | \{d_i\}\right) \propto \sum_{i=1}^{N_\mathrm{obs}} \log \bigg[ \frac{1}{K_i} \sum_{j=1}^{K_i} \frac{p(\theta^{i,j}|\Lambda)p(z^{i,j}|\lambda)}{\pi(\theta^{i,j}, z^{i,j})} \bigg] -  \\
    N_\mathrm{obs} \log \mu(\Lambda, \lambda) + \frac{3N_\mathrm{obs} + N_\mathrm{obs}^2}{2N_\mathrm{eff}} + \mathcal{O}(N_\mathrm{eff}^{-2}).
\end{equation}

We explicitly enumerate each of the models used in this work for $p(\theta|\Lambda)$, along with 
their respective hyperparameters and prior distributions in the next section. To calculate draw 
samples of the hyperparameters from the hierarchical posterior distribution shown in equation \ref{importance-posterior}, we use the 
NUTS Hamiltonian Monte Carlo sampler in \textsc{NumPyro} and \textsc{Jax} to calculate likelihoods \citep{jax,pyro,numpyro}.

\begin{table*}[b!]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Model} & \textbf{Parameter} & \textbf{Description} & \textbf{Prior} \\ \hline \hline
    \multicolumn{4}{|c|}{\textbf{Primary Mass Model Parameters}} \\ \hline
    \textsc{B-Spline Primary} & $\bm{c}$ & Basis coefficients & $\sim \mathrm{Smooth}(\tau_\lambda, \sigma, r, n)$ \\ \cline{2-4} 
     & $\tau_\lambda$ & Smoothing Prior Scale & $\sim \mathrm{U}(2,1000)$ \\ \cline{2-4}
     & $r$ & order of the difference matrix for the smoothing prior & 2 \\ \cline{2-4} 
     & $\sigma$ & width of Gaussian priors on coefficients in smoothing prior & 6 \\ \cline{2-4} 
     & $n$ & number of knots in the basis spline & 64 \\ \hline \hline 
    \multicolumn{4}{|c|}{\textbf{Mass Ratio Model Parameters}} \\ \hline
    \textsc{B-Spline Ratio} & $\bm{c}$ & Basis coefficients & $\sim \mathrm{Smooth}(\tau_\lambda, \sigma, r, n)$ \\ \cline{2-4} 
     & $\tau_\lambda$ & Smoothing Prior Scale & $\sim \mathrm{U}(1,100)$ \\ \cline{2-4}
     & $r$ & order of the difference matrix for the smoothing prior & 2 \\ \cline{2-4} 
     & $\sigma$ & width of Gaussian priors on coefficients in smoothing prior & 4 \\ \cline{2-4} 
     & $n$ & number of knots in the basis spline & 18 \\ \hline \hline
    \multicolumn{4}{|c|}{\textbf{Redshift Evolution Model Parameters}} \\ \hline
    \textsc{PowerLaw+B-Spline} & $\lambda$ & slope of redshift evolution power law $(1+z)^\lambda$ &  $\sim \mathcal{N}(0,3)$ \\ \cline{2-4}
    & $\bm{c}$ & Basis coefficients & $\sim \mathrm{Smooth}(\tau_\lambda, \sigma, r, n)$ \\ \cline{2-4} 
     & $\tau_\lambda$ & Smoothing Prior Scale & $\sim \mathrm{U}(1,10)$ \\ \cline{2-4}
     & $r$ & order of the difference matrix for the smoothing prior & 2 \\ \cline{2-4} 
     & $\sigma$ & width of Gaussian priors on coefficients in smoothing prior & 1 \\ \cline{2-4} 
     & $n$ & number of knots in the basis spline & 18 \\ \hline \hline 
    \multicolumn{4}{|c|}{\textbf{Spin Distribution Model Parameters}} \\ \hline
    \textsc{B-Spline Magnitude} & $\bm{c}$ &  Basis coefficients & $\sim \mathrm{Smooth}(\tau_\lambda, \sigma, r, n)$  \\ \cline{2-4} 
    & $\tau_\lambda$ & Smoothing Prior Scale & $\sim \mathrm{U}(1,10)$ \\ \cline{2-4}
    & $r$ & order of the difference matrix for the smoothing prior & 2 \\ \cline{2-4} 
    & $\sigma$ & width of Gaussian priors on coefficients in smoothing prior & 1 \\ \cline{2-4} 
    & $n$ & number of knots in the basis spline & 18 \\ \hline \hline 
    \textsc{B-Spline Tilt} & $\bm{c}$ &  Basis coefficients & $\sim \mathrm{Smooth}(\tau_\lambda, \sigma, r, n)$  \\ \cline{2-4} 
    & $\tau_\lambda$ & Smoothing Prior Scale & $\sim \mathrm{U}(1,10)$ \\ \cline{2-4}
    & $r$ & order of the difference matrix for the smoothing prior & 2 \\ \cline{2-4} 
    & $\sigma$ & width of Gaussian priors on coefficients in smoothing prior & 1 \\ \cline{2-4} 
    & $n$ & number of knots in the basis spline & 18 \\ \hline \hline
    \end{tabular}
    \caption{All hyperparameter prior choices for each of the newly introduced basis spline models from this manuscript. See appendix 
    \ref{sec:basis_splines} and \ref{sec:psplines} for more detailed description of basis spline or smoothing prior parameters.}
    \label{tab:model_priors}
\end{table*} 

\section{Model and Prior Specification} \label{sec:modelpriors}

For each of the distributions with basis spline distributions, we have 2 fixed hyperparameters to specify. 
The number of degrees of freedom, $n$, and the difference 
penalty order for the smoothing prior, $r$. Additionally, one must choose a prior distribution on the smoothing prior scale hyperparameter, 
$\tau_\lambda$, which we take to be Uniform. For the primary mass distribution we model the log probability with a B-Spline interpolated 
in $\log(m_1)$ space. We follow a similar scheme for the models in mass ratio and spin, except we model the log probability with 
B-Splines that are interpolated in $q$, $a_i$ or $\cos{\theta_i}$ space. We adopt a minimum black hole mass of $5\msun$, 
and maximum of $100\msun$ with the equally spaced in this range. The knots for the mass ratio B-Spline are equally spaced 
from $\frac{m_\mathrm{min}}{m_\mathrm{max}}=0.05$ to $1$. There is motivation for the evolution of the merger rate with redshift 
to follow a power law form since it should be related to the star formation rate \citep{Madau_2014}, 
motivating our adoption of a semi-parametric approach where we use B-Splines to model modulations to the 
simpler underlying \textsc{PowerlawRedshift} model \citep{Fishbach_2018redshift,Edelman_2022ApJ}. 
We model modulations to the underlying probability density with the multiplicative factor, $e^{B(\log z)}$, 
where $B(\log z)$ is the B-Spline interpolated from knots spaced linearly in $\log z$ space. 
We enumerate each of our specific model hyperparameter 
and prior choices in table \ref{tab:model_priors}.


\section{Posterior Predictive Checks} \label{sec:ppcs}

\begin{figure*}[ht!]
    \includegraphics[width=\linewidth]{figures/ppc_plot.pdf}
    \caption{Posterior predictive checks showing the CDFs of the observed (black) and predicted (red) distributions of GWTC-3 sized catalogs for each posterior sample of the IID spin B-Spline model. The shaded regions show 90\% credible intervals and the solid red line is the median of the predicted distribution.
    \label{fig:ppc}
    \script{plot_ppcs.py}
\end{figure*}

We follow the posterior predictive checking procedure done in recent population studies to validate our models inferences \citep{o3a_pop,Edelman_2022ApJ}. 
For each posterior sample describing our model's inferred population we reweigh the observed event samples and the found injections to that population and draw a set 69 (size of GWTC-3 BBH catalog) samples to construct the observed and predicted distributions we show in \ref{fig:ppc}. When the observed region stays encompassed within the predicted region the model is performing well, which we see across each of the fit parameters. 