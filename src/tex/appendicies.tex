
\appendix
\section{Hierarchical Bayesian Inference} \label{sec:hierarchical_inference}

% This appendix goes over in detail the hierarchical bayeisan inference framework
We use hierarchical Bayesian inference to infer the population properties of CBCs. We want to infer the number density of merging CBCs 
in the universe and how this can change with their masses, spins, etc. Often times it is useful to formulate the question in terms of the 
merger rates which is the number of mergers per $Gpc^{3}$ co-moving volume per year. For a set of hyper-parameters, $\Lambda$, $\lambda$, and local ($z=0$) 
merger rate density, $\mathcal{R}_0$, we write the overall number density of BBH mergers in the universe as: 

\begin{equation} \label{number_density}
     \frac{dN(\theta, z | \mathcal{R}_0, \Lambda, \lambda)}{d\theta dz} = \frac{dV_c}{dz}\bigg(\frac{T_\mathrm{obs}}{1+z}\bigg) \frac{d\mathcal{R}(\theta, z | \mathcal{R}_0, \Lambda, \lambda)}{d\theta} = \mathcal{R}_0 p(\theta | \Lambda) p(z | \lambda)
\end{equation}

\noindent
Where up above, we denote the co-moving volume element as $dV_c$ \citep{hogg_cosmo}, and $T_\mathrm{obs}$ as the observing time period that produced the 
catalog with the related factor of $1+z$ converting this detector-frame time to source-frame. We assume a LambdaCDM cosmology using 
the cosmological parameters from \citet{Planck2015}. We model the merger rate evolving with redshift following a power law distribution: 
$p(z|\lambda) \propto \frac{dV_c}{dz}\frac{1}{1+z}(1+z)^\lambda$. When integrating equation \ref{number_density} across all $\theta$
and out to some maximum redshift, $z_\mathrm{max}$, we get the total number of CBCs in the universe out to that redshift. We follow previous notations, \
letting $\{d_i\}$ represent the set of data from $N_\mathrm{obs}$ CBCs observed with gravitational waves. The merger rate is then described as an inhomogenous 
Poisson process and after imposing the usual log-uniform prior on the merger rate, we marginalize out merger rate and arrive at the posterior
distribution of our hyper-parameters, $\Lambda$ \citep{Mandel_2019, Vitale_2021}.

\begin{equation}
    p\left(\Lambda, \lambda | \{d_i\}\right) \frac{p(\Lambda)p(\lambda)}{\xi(\Lambda,\lambda)^{N_\mathrm{obs}}} \prod_{i=1}^{N_\mathrm{obs}} \bigg[ \frac{1}{K_i} \sum_{j=1}^{K_i} \frac{p(\theta^{i,j}|\Lambda)p(z^{i,j}|\lambda)}{\pi(\theta, z^{i,j})} \bigg]
\end{equation}

\noindent
where, we replaced the integrals over each event's likelihood with ensemble averages over $K_i$ posterior samples. Above, $j$
indexes the $K_i$ posterior samples from each event and $\pi(\theta, z)$ is the default prior used by parameter estimations that 
produced the posterior samples for each event. In the analyses of GWTC-2 the default prior used is uniform in detector frame masses, 
component spins and Euclidean volume. The corresponding prior evaluated in source frame primary mass, mass ratio, component spins 
and redshift is:

\begin{equation}
    \pi(m_1, q, a_1, a_2, \cos{\theta_1}, \cos{\theta_2}, z) \propto \frac{1}{4} m_1 (1+z)^2 D_L^2(z) \frac{dD_L}{dz}
\end{equation}

\noindent where $D_L$ is the luminosity distance. To carefully incorporate selection effects to our model we need to quantify the detection efficiency,
$\xi(\Lambda, \lambda)$, of the search pipelines that were used to create GWTC-3, at a given population distribution described by $\Lambda$ and $\lambda$.
 
\begin{equation}
     \xi(\Lambda, \lambda) = \int d\theta dz P_\mathrm{det}(\theta, z)p(\theta | \Lambda) p(z | \lambda)
\end{equation}
 
\noindent
To estimate this integral we use a software injection campaign where gravitational waveforms from a large population of simulated sources. 
These simulated waevforms are put into real detector data, and then this data is evaluated with the same search pipelines that were used to 
produce the catalog we are analyzing. With these search results in hand, we use importance sampling and evaluate the integral 
with the monte carlo sum estimate $\mu$, and its corresponding variance and effective number of samples:

\begin{equation} \label{xi}
     \xi(\Lambda, \lambda) \approx \mu(\Lambda, \lambda) \frac{1}{N_\mathrm{inj}} \sum_{i=1}^{N_\mathrm{found}} \frac{p(\theta^i | \Lambda) p(z^i | \lambda)}{p_\mathrm{inj}(\theta, z^i)}
\end{equation}

\begin{equation}
    \sigma^2(\Lambda, \lambda) \equiv \frac{\mu^2(\Lambda, \lambda)}{N_\mathrm{eff}} \simeq \frac{1}{N^2_\mathrm{inj}} \sum_{i=1}^{N_\mathrm{found}} \bigg[\frac{p(\theta | \Lambda) p(z | \lambda)}{p_\mathrm{inj}(\theta, z)}\bigg]^2 - \frac{\mu^2(\Lambda, \lambda)}{N_\mathrm{inj}}
\end{equation}

\noindent
Where the sums indexes only over the $N_\mathrm{found}$ injections that were successfully detected out of $N_\mathrm{inj}$ total injections, 
and $p_\mathrm{inj}(\theta, z)$ is the reference distribution from which the injections were drawn. Additionally, we follow the procedure 
outlined in \citet{Farr_2019} to marginalize the uncertainty in our estimate of $\xi(\Lambda, \lambda)$, in which we verify that $N_\mathrm{eff}$ is 
sufficiently high after re-weighting the injections to a given population (i.e. $N_\mathrm{eff} > 4N_\mathrm{obs}$). 
The total hyper-posterior marginalized over the merger rate and the uncertainty in the monte carlo integral calculating $\xi(\Lambda, \lambda)$ \citep{Farr_2019}, as:

\begin{equation}\label{importance-posterior}
    \log p\left(\Lambda, \lambda | \{d_i\}\right) \propto \sum_{i=1}^{N_\mathrm{obs}} \log \bigg[ \frac{1}{K_i} \sum_{j=1}^{K_i} \frac{p(\theta^{i,j}|\Lambda)p(z^{i,j}|\lambda)}{\pi(\theta^{i,j}, z^{i,j})} \bigg] -  \\
    N_\mathrm{obs} \log \mu(\Lambda, \lambda) + \frac{3N_\mathrm{obs} + N_\mathrm{obs}^2}{2N_\mathrm{eff}} + \mathcal{O}(N_\mathrm{eff}^{-2}).
\end{equation}

We explicitly enumerate each of the models used in this work for $p(\theta|\Lambda)$, along with 
their respective hyper-parameters and prior distributions in the next section. To calculate marginal likelihoods and draw 
samples of the hyper parameters from the hierarchical posterior distribution shown in equation \ref{importance-posterior}, we use the 
\textsc{Bilby} \citep{Ashton_2019, bilby_gwtc1} and \textsc{GWPopulation} \citep{Talbot_2019} Bayesian inference software libraries with 
the \textsc{Dynesty} dynamic nested sampling algorithm \citep{Speagle_2020}. We also make use of the hamiltonian monte carlo algorithms in 
\textsc{numpyro} and use \textsc{jax} to calculate our likelihoods \bruce{FIND citations for these}.

\section{Models and Hyper-Parameters} \label{sec:hypparams}

\section{Validation Studies} \label{sec:validation}